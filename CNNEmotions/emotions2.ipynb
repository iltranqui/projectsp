{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotions and CNN Project with ImageGenerator\n",
    "\n",
    "Goal: create an CNN model which can predict the emotions and use your own pictures to actually recognize your emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\enric\\Documents\\projectsp\\CNNEmotions\\heavy\n"
     ]
    }
   ],
   "source": [
    "# get the path of the current directory\n",
    "path = os.getcwd()\n",
    "path = os.path.join(path,'heavy')\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'heavy/fer2013.csv.zip'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22768\\4132740452.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# open the zip file in read mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mZipFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzippet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[1;31m# list all the contents\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mzip\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprintdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ludo\\lib\\zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, file, mode, compression, allowZip64, compresslevel)\u001b[0m\n\u001b[0;32m   1238\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1239\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1240\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilemode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1241\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1242\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mfilemode\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodeDict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'heavy/fer2013.csv.zip'"
     ]
    }
   ],
   "source": [
    "# Extracting the zipfile \n",
    "from zipfile import ZipFile\n",
    "\n",
    "# specifity the name of the file\n",
    "zippet = str(\"fer2013.csv.zip\")\n",
    "\n",
    "# open the zip file in read mode\n",
    "with ZipFile(zippet, 'r') as zip:\n",
    "    # list all the contents\n",
    "    zip.printdir()\n",
    "\n",
    "    # extract all the files\n",
    "    print('extraction...')\n",
    "    zip.extractall()\n",
    "    print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Who can understand anything of this ? it is RGB images with 48x48 pixels. \n",
    "\n",
    "Can i trasfomr the 1st row into an image ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 35887 entries, 0 to 35886\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   emotion  35887 non-null  int64 \n",
      " 1   pixels   35887 non-null  object\n",
      " 2   Usage    35887 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 841.2+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "face = pd.read_csv('fer2013.csv')\n",
    "face.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2304"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1st row and the pixels image\n",
    "face.loc[0,'pixels'];\n",
    "# type(face.loc[0,'pixels'])\n",
    "image1 = list(map(int, face.loc[0,'pixels'].split(\" \")))\n",
    "len(image1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as we can see the pixels are a 48x48 image since the len is 2304 = 48x48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class:  ndarray\n",
      "shape:  (48, 48, 1)\n",
      "strides:  (48, 1, 1)\n",
      "itemsize:  1\n",
      "aligned:  True\n",
      "contiguous:  True\n",
      "fortran:  False\n",
      "data pointer: 0x19d230a2d30\n",
      "byteorder:  little\n",
      "byteswap:  False\n",
      "type: uint8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x19d131e2548>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "\n",
    "# Open image file, slurp the lot\n",
    "# contents = Path(image1).read_text()\n",
    "\n",
    "width = 48;\n",
    "height = 48;\n",
    "\n",
    "# Make a list of anything that looks like numbers using a regex...\n",
    "# ... taking first as height, second as width and remainder as pixels\n",
    "# h, w *pixels = re.findall(r'[0-9]+', contents)\n",
    "\n",
    "# Now make pixels into Numpy array of uint8 and reshape to correct height, width and depth\n",
    "na = np.array(image1, dtype=np.uint8).reshape((int(48),int(48),1))\n",
    "\n",
    "np.info(na);\n",
    "# Now make the Numpy array into a PIL Image and save\n",
    "#Image.fromarray(na).save(\"result.png\")\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(na, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok, this dataset is in greyscale, so i need to go and download the original one ! at the folliwing site:\n",
    "\n",
    "https://www.kaggle.com/datasets/deadskull7/fer2013\n",
    "\n",
    "i was wrong. it is a greyscale images that i have to train in order to understand the emotions. Next step is to separete them into training and testing using the label in the 3rd column. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0           Training\n",
      "1           Training\n",
      "2           Training\n",
      "3           Training\n",
      "4           Training\n",
      "            ...     \n",
      "35882    PrivateTest\n",
      "35883    PrivateTest\n",
      "35884    PrivateTest\n",
      "35885    PrivateTest\n",
      "35886    PrivateTest\n",
      "Name: Usage, Length: 35887, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>pixels</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Usage</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>PrivateTest</th>\n",
       "      <td>3589</td>\n",
       "      <td>3589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PublicTest</th>\n",
       "      <td>3589</td>\n",
       "      <td>3589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Training</th>\n",
       "      <td>28709</td>\n",
       "      <td>28709</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             emotion  pixels\n",
       "Usage                       \n",
       "PrivateTest     3589    3589\n",
       "PublicTest      3589    3589\n",
       "Training       28709   28709"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(face.loc[:,'Usage']);\n",
    "face.groupby('Usage').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's place all the Public and Private test in the same test set and let's see how many classification emotions we have at the end, in order to see up the last layer with an appropriate number of nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" def count(mylist):\n",
    "    return len(set(myList))\n",
    "\n",
    "print(count(face[\"emotion\"]))   \"\"\"     \n",
    "\n",
    "len(set(face[\"emotion\"]))\n",
    "\n",
    "# so based on the result there are 7 different emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 28709 entries, 0 to 28708\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   emotion  28709 non-null  int64 \n",
      " 1   pixels   28709 non-null  object\n",
      " 2   Usage    28709 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 897.2+ KB\n"
     ]
    }
   ],
   "source": [
    "# Divide the training into training and test set\n",
    "values = ['PublicTest', 'PrivateTest']\n",
    "T = face[face['Usage'].isin(values) == False]\n",
    "\n",
    "T.groupby('Usage').count();\n",
    "T.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "# let's create the X_train in another way\n",
    "new_pixels = []\n",
    "print(type(T[\"pixels\"]))\n",
    "X_train = np.asarray(T[\"pixels\"])\n",
    "\n",
    "for row in X_train:\n",
    "    row = np.asarray(list(map(int, row.split(\" \"))))\n",
    "    new_pixels.append(row.reshape(48,48)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(new_pixels, T['emotion'], test_size=0.2, random_state=35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN to train this test set. \n",
    "\n",
    "Use a CNN neural network to predict the images\n",
    "\n",
    "https://www.edureka.co/blog/convolutional-neural-network/\n",
    "\n",
    "See "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.asarray(X_train)\n",
    "X_train = X_train.reshape(-1, 48, 48, 1)\n",
    "X_test = np.asarray(X_test)\n",
    "X_test = X_test.reshape(-1, 48, 48, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import InputLayer, Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
    "from tensorflow.keras.optimizers import RMSprop,Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# take as input \n",
    "model.add(Conv2D(filters = 8, kernel_size = (5,5),padding = 'Same',activation ='relu', input_shape = (48,48,1)))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Conv2D(filters = 16, kernel_size = (3,3),padding = 'Same', activation ='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
    "# fully connected\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation = \"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(7, activation = \"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimizer\n",
    "optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer = optimizer , loss = \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# sparse_categorical_crossentropy = essential for multiple classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200  # for better result increase the epochs\n",
    "batch_size = 250"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation\n",
    "To avoid overfitting problem, we need to expand artificially our handwritten digit dataset\n",
    "Alter the training data with small transformations to reproduce the variations of digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # dimesion reduction\n",
    "        rotation_range=5,  # randomly rotate images in the range 5 degrees\n",
    "        zoom_range = 0.1, # Randomly zoom image 10%\n",
    "        width_shift_range=0.1,  # randomly shift images horizontally 10%\n",
    "        height_shift_range=0.1,  # randomly shift images vertically 10%\n",
    "        horizontal_flip=False,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "92/92 [==============================] - 23s 252ms/step - loss: 1.2043 - accuracy: 0.5455\n",
      "Epoch 2/200\n",
      "92/92 [==============================] - 25s 275ms/step - loss: 1.2142 - accuracy: 0.5386\n",
      "Epoch 3/200\n",
      "92/92 [==============================] - 24s 259ms/step - loss: 1.2101 - accuracy: 0.5407\n",
      "Epoch 4/200\n",
      "92/92 [==============================] - 26s 282ms/step - loss: 1.2114 - accuracy: 0.5419\n",
      "Epoch 5/200\n",
      "92/92 [==============================] - 30s 326ms/step - loss: 1.2095 - accuracy: 0.5373\n",
      "Epoch 6/200\n",
      "92/92 [==============================] - 30s 330ms/step - loss: 1.2133 - accuracy: 0.5449\n",
      "Epoch 7/200\n",
      "92/92 [==============================] - 28s 307ms/step - loss: 1.2138 - accuracy: 0.5384\n",
      "Epoch 8/200\n",
      "92/92 [==============================] - 26s 277ms/step - loss: 1.2138 - accuracy: 0.5388\n",
      "Epoch 9/200\n",
      "92/92 [==============================] - 26s 286ms/step - loss: 1.2107 - accuracy: 0.5395\n",
      "Epoch 10/200\n",
      "92/92 [==============================] - 30s 326ms/step - loss: 1.2028 - accuracy: 0.5424\n",
      "Epoch 11/200\n",
      "92/92 [==============================] - 29s 316ms/step - loss: 1.1997 - accuracy: 0.5451\n",
      "Epoch 12/200\n",
      "92/92 [==============================] - 28s 309ms/step - loss: 1.2193 - accuracy: 0.5384\n",
      "Epoch 13/200\n",
      "92/92 [==============================] - 29s 313ms/step - loss: 1.2077 - accuracy: 0.5450\n",
      "Epoch 14/200\n",
      "92/92 [==============================] - 30s 321ms/step - loss: 1.2057 - accuracy: 0.5437\n",
      "Epoch 15/200\n",
      "92/92 [==============================] - 28s 303ms/step - loss: 1.2030 - accuracy: 0.5450\n",
      "Epoch 16/200\n",
      "92/92 [==============================] - 26s 288ms/step - loss: 1.2099 - accuracy: 0.5465\n",
      "Epoch 17/200\n",
      "92/92 [==============================] - 27s 297ms/step - loss: 1.2137 - accuracy: 0.5427\n",
      "Epoch 18/200\n",
      "92/92 [==============================] - 26s 282ms/step - loss: 1.2040 - accuracy: 0.5442\n",
      "Epoch 19/200\n",
      "92/92 [==============================] - 25s 273ms/step - loss: 1.2008 - accuracy: 0.5436\n",
      "Epoch 20/200\n",
      "92/92 [==============================] - 27s 293ms/step - loss: 1.2120 - accuracy: 0.5409\n",
      "Epoch 21/200\n",
      "92/92 [==============================] - 26s 285ms/step - loss: 1.2057 - accuracy: 0.5388\n",
      "Epoch 22/200\n",
      "92/92 [==============================] - 28s 299ms/step - loss: 1.2109 - accuracy: 0.5430\n",
      "Epoch 23/200\n",
      "92/92 [==============================] - 26s 281ms/step - loss: 1.2029 - accuracy: 0.5384\n",
      "Epoch 24/200\n",
      "92/92 [==============================] - 26s 280ms/step - loss: 1.2029 - accuracy: 0.5455\n",
      "Epoch 25/200\n",
      "92/92 [==============================] - 27s 299ms/step - loss: 1.2042 - accuracy: 0.5445\n",
      "Epoch 26/200\n",
      "92/92 [==============================] - 27s 291ms/step - loss: 1.2023 - accuracy: 0.5458\n",
      "Epoch 27/200\n",
      "92/92 [==============================] - 27s 291ms/step - loss: 1.2037 - accuracy: 0.5406\n",
      "Epoch 28/200\n",
      "92/92 [==============================] - 28s 299ms/step - loss: 1.2093 - accuracy: 0.5413\n",
      "Epoch 29/200\n",
      "92/92 [==============================] - 32s 345ms/step - loss: 1.1926 - accuracy: 0.5484\n",
      "Epoch 30/200\n",
      "92/92 [==============================] - 28s 301ms/step - loss: 1.1995 - accuracy: 0.5442\n",
      "Epoch 31/200\n",
      "92/92 [==============================] - 29s 316ms/step - loss: 1.2019 - accuracy: 0.5464\n",
      "Epoch 32/200\n",
      "92/92 [==============================] - 30s 328ms/step - loss: 1.1970 - accuracy: 0.5424\n",
      "Epoch 33/200\n",
      "92/92 [==============================] - 26s 280ms/step - loss: 1.2039 - accuracy: 0.5449\n",
      "Epoch 34/200\n",
      "92/92 [==============================] - 27s 290ms/step - loss: 1.2019 - accuracy: 0.5439\n",
      "Epoch 35/200\n",
      "92/92 [==============================] - 27s 296ms/step - loss: 1.2024 - accuracy: 0.5478\n",
      "Epoch 36/200\n",
      "92/92 [==============================] - 27s 298ms/step - loss: 1.1952 - accuracy: 0.5443\n",
      "Epoch 37/200\n",
      "92/92 [==============================] - 28s 299ms/step - loss: 1.1978 - accuracy: 0.5423\n",
      "Epoch 38/200\n",
      "92/92 [==============================] - 27s 297ms/step - loss: 1.1941 - accuracy: 0.5469\n",
      "Epoch 39/200\n",
      "92/92 [==============================] - 28s 302ms/step - loss: 1.2028 - accuracy: 0.5396\n",
      "Epoch 40/200\n",
      "92/92 [==============================] - 27s 292ms/step - loss: 1.1975 - accuracy: 0.5453\n",
      "Epoch 41/200\n",
      "92/92 [==============================] - 26s 281ms/step - loss: 1.1967 - accuracy: 0.5474\n",
      "Epoch 42/200\n",
      "92/92 [==============================] - 26s 285ms/step - loss: 1.2023 - accuracy: 0.5452\n",
      "Epoch 43/200\n",
      "92/92 [==============================] - 28s 304ms/step - loss: 1.1927 - accuracy: 0.5495\n",
      "Epoch 44/200\n",
      "92/92 [==============================] - 26s 283ms/step - loss: 1.2004 - accuracy: 0.5449\n",
      "Epoch 45/200\n",
      "92/92 [==============================] - 27s 294ms/step - loss: 1.1928 - accuracy: 0.5479\n",
      "Epoch 46/200\n",
      "92/92 [==============================] - 28s 305ms/step - loss: 1.2110 - accuracy: 0.5431\n",
      "Epoch 47/200\n",
      "92/92 [==============================] - 27s 293ms/step - loss: 1.1976 - accuracy: 0.5423\n",
      "Epoch 48/200\n",
      "92/92 [==============================] - 25s 273ms/step - loss: 1.1965 - accuracy: 0.5473\n",
      "Epoch 49/200\n",
      "92/92 [==============================] - 26s 285ms/step - loss: 1.1970 - accuracy: 0.5457\n",
      "Epoch 50/200\n",
      "92/92 [==============================] - 27s 295ms/step - loss: 1.1933 - accuracy: 0.5483\n",
      "Epoch 51/200\n",
      "92/92 [==============================] - 28s 302ms/step - loss: 1.1890 - accuracy: 0.5502\n",
      "Epoch 52/200\n",
      "92/92 [==============================] - 25s 277ms/step - loss: 1.1980 - accuracy: 0.5485\n",
      "Epoch 53/200\n",
      "92/92 [==============================] - 27s 289ms/step - loss: 1.1920 - accuracy: 0.5509\n",
      "Epoch 54/200\n",
      "92/92 [==============================] - 28s 302ms/step - loss: 1.1861 - accuracy: 0.5561\n",
      "Epoch 55/200\n",
      "92/92 [==============================] - 28s 301ms/step - loss: 1.2054 - accuracy: 0.5425\n",
      "Epoch 56/200\n",
      "92/92 [==============================] - 25s 276ms/step - loss: 1.1981 - accuracy: 0.5482\n",
      "Epoch 57/200\n",
      "92/92 [==============================] - 28s 300ms/step - loss: 1.1852 - accuracy: 0.5521\n",
      "Epoch 58/200\n",
      "92/92 [==============================] - 29s 313ms/step - loss: 1.1866 - accuracy: 0.5489\n",
      "Epoch 59/200\n",
      "92/92 [==============================] - 26s 279ms/step - loss: 1.1893 - accuracy: 0.5508\n",
      "Epoch 60/200\n",
      "92/92 [==============================] - 27s 295ms/step - loss: 1.1923 - accuracy: 0.5468\n",
      "Epoch 61/200\n",
      "92/92 [==============================] - 28s 310ms/step - loss: 1.1931 - accuracy: 0.5475\n",
      "Epoch 62/200\n",
      "92/92 [==============================] - 27s 291ms/step - loss: 1.1885 - accuracy: 0.5485\n",
      "Epoch 63/200\n",
      "92/92 [==============================] - 25s 275ms/step - loss: 1.1945 - accuracy: 0.5477\n",
      "Epoch 64/200\n",
      "92/92 [==============================] - 27s 295ms/step - loss: 1.1935 - accuracy: 0.5503\n",
      "Epoch 65/200\n",
      "92/92 [==============================] - 29s 313ms/step - loss: 1.1824 - accuracy: 0.5508\n",
      "Epoch 66/200\n",
      "92/92 [==============================] - 27s 297ms/step - loss: 1.1826 - accuracy: 0.5526\n",
      "Epoch 67/200\n",
      "92/92 [==============================] - 26s 281ms/step - loss: 1.1878 - accuracy: 0.5504\n",
      "Epoch 68/200\n",
      "92/92 [==============================] - 27s 291ms/step - loss: 1.2007 - accuracy: 0.5437\n",
      "Epoch 69/200\n",
      "92/92 [==============================] - 28s 305ms/step - loss: 1.1878 - accuracy: 0.5509\n",
      "Epoch 70/200\n",
      "92/92 [==============================] - 27s 298ms/step - loss: 1.1906 - accuracy: 0.5488\n",
      "Epoch 71/200\n",
      "92/92 [==============================] - 28s 300ms/step - loss: 1.2019 - accuracy: 0.5494\n",
      "Epoch 72/200\n",
      "92/92 [==============================] - 26s 278ms/step - loss: 1.1879 - accuracy: 0.5495\n",
      "Epoch 73/200\n",
      "92/92 [==============================] - 26s 279ms/step - loss: 1.1930 - accuracy: 0.5500\n",
      "Epoch 74/200\n",
      "92/92 [==============================] - 28s 300ms/step - loss: 1.1841 - accuracy: 0.5488\n",
      "Epoch 75/200\n",
      "92/92 [==============================] - 27s 297ms/step - loss: 1.1926 - accuracy: 0.5497\n",
      "Epoch 76/200\n",
      "92/92 [==============================] - 27s 299ms/step - loss: 1.1901 - accuracy: 0.5468\n",
      "Epoch 77/200\n",
      "92/92 [==============================] - 27s 289ms/step - loss: 1.1853 - accuracy: 0.5519\n",
      "Epoch 78/200\n",
      "92/92 [==============================] - 28s 304ms/step - loss: 1.1800 - accuracy: 0.5553\n",
      "Epoch 79/200\n",
      "92/92 [==============================] - 29s 317ms/step - loss: 1.1972 - accuracy: 0.5473\n",
      "Epoch 80/200\n",
      "92/92 [==============================] - 29s 319ms/step - loss: 1.1855 - accuracy: 0.5503\n",
      "Epoch 81/200\n",
      "92/92 [==============================] - 31s 339ms/step - loss: 1.1831 - accuracy: 0.5537\n",
      "Epoch 82/200\n",
      "92/92 [==============================] - 28s 301ms/step - loss: 1.1741 - accuracy: 0.5564\n",
      "Epoch 83/200\n",
      "92/92 [==============================] - 27s 292ms/step - loss: 1.1969 - accuracy: 0.5465\n",
      "Epoch 84/200\n",
      "92/92 [==============================] - 27s 297ms/step - loss: 1.1851 - accuracy: 0.5477\n",
      "Epoch 85/200\n",
      "92/92 [==============================] - 25s 277ms/step - loss: 1.1877 - accuracy: 0.5524\n",
      "Epoch 86/200\n",
      "92/92 [==============================] - 26s 283ms/step - loss: 1.1777 - accuracy: 0.5577\n",
      "Epoch 87/200\n",
      "92/92 [==============================] - 25s 275ms/step - loss: 1.1876 - accuracy: 0.5568\n",
      "Epoch 88/200\n",
      "92/92 [==============================] - 26s 278ms/step - loss: 1.1915 - accuracy: 0.5471\n",
      "Epoch 89/200\n",
      "92/92 [==============================] - 28s 302ms/step - loss: 1.1911 - accuracy: 0.5491\n",
      "Epoch 90/200\n",
      "92/92 [==============================] - 27s 292ms/step - loss: 1.1932 - accuracy: 0.5446\n",
      "Epoch 91/200\n",
      "92/92 [==============================] - 23s 249ms/step - loss: 1.1857 - accuracy: 0.5518\n",
      "Epoch 92/200\n",
      "92/92 [==============================] - 26s 280ms/step - loss: 1.1753 - accuracy: 0.5525\n",
      "Epoch 93/200\n",
      "92/92 [==============================] - 25s 271ms/step - loss: 1.1787 - accuracy: 0.5552\n",
      "Epoch 94/200\n",
      "92/92 [==============================] - 25s 268ms/step - loss: 1.1892 - accuracy: 0.5498\n",
      "Epoch 95/200\n",
      "92/92 [==============================] - 27s 293ms/step - loss: 1.1925 - accuracy: 0.5483\n",
      "Epoch 96/200\n",
      "92/92 [==============================] - 26s 278ms/step - loss: 1.1850 - accuracy: 0.5520\n",
      "Epoch 97/200\n",
      "92/92 [==============================] - 25s 266ms/step - loss: 1.1847 - accuracy: 0.5515\n",
      "Epoch 98/200\n",
      "92/92 [==============================] - 21s 232ms/step - loss: 1.1854 - accuracy: 0.5531\n",
      "Epoch 99/200\n",
      "92/92 [==============================] - 20s 218ms/step - loss: 1.1917 - accuracy: 0.5501\n",
      "Epoch 100/200\n",
      "92/92 [==============================] - 22s 238ms/step - loss: 1.1790 - accuracy: 0.5548\n",
      "Epoch 101/200\n",
      "92/92 [==============================] - 25s 267ms/step - loss: 1.1781 - accuracy: 0.5544\n",
      "Epoch 102/200\n",
      "92/92 [==============================] - 24s 263ms/step - loss: 1.1819 - accuracy: 0.5557\n",
      "Epoch 103/200\n",
      "92/92 [==============================] - 24s 257ms/step - loss: 1.1801 - accuracy: 0.5529\n",
      "Epoch 104/200\n",
      "92/92 [==============================] - 26s 284ms/step - loss: 1.1741 - accuracy: 0.5591\n",
      "Epoch 105/200\n",
      "92/92 [==============================] - 27s 294ms/step - loss: 1.1750 - accuracy: 0.5580\n",
      "Epoch 106/200\n",
      "92/92 [==============================] - 27s 295ms/step - loss: 1.1735 - accuracy: 0.5568\n",
      "Epoch 107/200\n",
      "92/92 [==============================] - 26s 283ms/step - loss: 1.1654 - accuracy: 0.5567\n",
      "Epoch 108/200\n",
      "92/92 [==============================] - 28s 301ms/step - loss: 1.1921 - accuracy: 0.5552\n",
      "Epoch 109/200\n",
      "92/92 [==============================] - 30s 331ms/step - loss: 1.1731 - accuracy: 0.5549\n",
      "Epoch 110/200\n",
      "92/92 [==============================] - 28s 309ms/step - loss: 1.1805 - accuracy: 0.5542\n",
      "Epoch 111/200\n",
      "92/92 [==============================] - 26s 286ms/step - loss: 1.1848 - accuracy: 0.5517\n",
      "Epoch 112/200\n",
      "92/92 [==============================] - 24s 262ms/step - loss: 1.1788 - accuracy: 0.5565\n",
      "Epoch 113/200\n",
      "92/92 [==============================] - 26s 286ms/step - loss: 1.1676 - accuracy: 0.5597\n",
      "Epoch 114/200\n",
      "92/92 [==============================] - 24s 259ms/step - loss: 1.1848 - accuracy: 0.5529\n",
      "Epoch 115/200\n",
      "92/92 [==============================] - 24s 259ms/step - loss: 1.1934 - accuracy: 0.5462\n",
      "Epoch 116/200\n",
      "92/92 [==============================] - 25s 276ms/step - loss: 1.1787 - accuracy: 0.5482\n",
      "Epoch 117/200\n",
      "92/92 [==============================] - 24s 263ms/step - loss: 1.1771 - accuracy: 0.5572\n",
      "Epoch 118/200\n",
      "92/92 [==============================] - 25s 267ms/step - loss: 1.1907 - accuracy: 0.5496\n",
      "Epoch 119/200\n",
      "92/92 [==============================] - 30s 326ms/step - loss: 1.1816 - accuracy: 0.5571\n",
      "Epoch 120/200\n",
      "92/92 [==============================] - 29s 321ms/step - loss: 1.1770 - accuracy: 0.5552\n",
      "Epoch 121/200\n",
      "92/92 [==============================] - 25s 271ms/step - loss: 1.1762 - accuracy: 0.5564\n",
      "Epoch 122/200\n",
      "92/92 [==============================] - 26s 285ms/step - loss: 1.1819 - accuracy: 0.5529\n",
      "Epoch 123/200\n",
      "92/92 [==============================] - 27s 294ms/step - loss: 1.1669 - accuracy: 0.5604\n",
      "Epoch 124/200\n",
      "92/92 [==============================] - 22s 243ms/step - loss: 1.1766 - accuracy: 0.5565\n",
      "Epoch 125/200\n",
      "92/92 [==============================] - 20s 221ms/step - loss: 1.1721 - accuracy: 0.5561\n",
      "Epoch 126/200\n",
      "92/92 [==============================] - 20s 217ms/step - loss: 1.1751 - accuracy: 0.5575\n",
      "Epoch 127/200\n",
      "92/92 [==============================] - 21s 230ms/step - loss: 1.1749 - accuracy: 0.5553\n",
      "Epoch 128/200\n",
      "92/92 [==============================] - 23s 246ms/step - loss: 1.1736 - accuracy: 0.5591s - loss: 1.1736 - accuracy: 0.55\n",
      "Epoch 129/200\n",
      "92/92 [==============================] - 20s 216ms/step - loss: 1.1709 - accuracy: 0.5601\n",
      "Epoch 130/200\n",
      "92/92 [==============================] - 21s 226ms/step - loss: 1.1779 - accuracy: 0.5542\n",
      "Epoch 131/200\n",
      "92/92 [==============================] - 26s 277ms/step - loss: 1.1704 - accuracy: 0.5579\n",
      "Epoch 132/200\n",
      "92/92 [==============================] - 27s 293ms/step - loss: 1.1694 - accuracy: 0.5590\n",
      "Epoch 133/200\n",
      "92/92 [==============================] - 31s 341ms/step - loss: 1.1727 - accuracy: 0.5575\n",
      "Epoch 134/200\n",
      "92/92 [==============================] - 35s 375ms/step - loss: 1.1845 - accuracy: 0.5518\n",
      "Epoch 135/200\n",
      "92/92 [==============================] - 33s 363ms/step - loss: 1.1735 - accuracy: 0.5526\n",
      "Epoch 136/200\n",
      "92/92 [==============================] - 36s 395ms/step - loss: 1.1731 - accuracy: 0.5579\n",
      "Epoch 137/200\n",
      "92/92 [==============================] - 27s 293ms/step - loss: 1.1725 - accuracy: 0.5589\n",
      "Epoch 138/200\n",
      "92/92 [==============================] - 27s 290ms/step - loss: 1.1640 - accuracy: 0.5581\n",
      "Epoch 139/200\n",
      "92/92 [==============================] - 30s 329ms/step - loss: 1.1729 - accuracy: 0.5580\n",
      "Epoch 140/200\n",
      "92/92 [==============================] - 27s 297ms/step - loss: 1.1756 - accuracy: 0.5566\n",
      "Epoch 141/200\n",
      "92/92 [==============================] - 23s 252ms/step - loss: 1.1629 - accuracy: 0.5571\n",
      "Epoch 142/200\n",
      "92/92 [==============================] - 23s 250ms/step - loss: 1.1676 - accuracy: 0.5610\n",
      "Epoch 143/200\n",
      "92/92 [==============================] - 21s 227ms/step - loss: 1.1710 - accuracy: 0.5599\n",
      "Epoch 144/200\n",
      "92/92 [==============================] - 24s 256ms/step - loss: 1.1680 - accuracy: 0.5596\n",
      "Epoch 145/200\n",
      "92/92 [==============================] - 22s 237ms/step - loss: 1.1762 - accuracy: 0.5567\n",
      "Epoch 146/200\n",
      "92/92 [==============================] - 22s 243ms/step - loss: 1.1741 - accuracy: 0.5559\n",
      "Epoch 147/200\n",
      "92/92 [==============================] - 23s 251ms/step - loss: 1.1568 - accuracy: 0.5646\n",
      "Epoch 148/200\n",
      "92/92 [==============================] - 25s 277ms/step - loss: 1.1787 - accuracy: 0.5529\n",
      "Epoch 149/200\n",
      "92/92 [==============================] - 23s 253ms/step - loss: 1.1703 - accuracy: 0.5560\n",
      "Epoch 150/200\n",
      "92/92 [==============================] - 24s 261ms/step - loss: 1.1746 - accuracy: 0.5561\n",
      "Epoch 151/200\n",
      "92/92 [==============================] - 25s 273ms/step - loss: 1.1695 - accuracy: 0.5568\n",
      "Epoch 152/200\n",
      "92/92 [==============================] - 25s 267ms/step - loss: 1.1761 - accuracy: 0.5567\n",
      "Epoch 153/200\n",
      "92/92 [==============================] - 24s 257ms/step - loss: 1.1622 - accuracy: 0.5588\n",
      "Epoch 154/200\n",
      "92/92 [==============================] - 25s 275ms/step - loss: 1.1728 - accuracy: 0.5553\n",
      "Epoch 155/200\n",
      "92/92 [==============================] - 23s 248ms/step - loss: 1.1618 - accuracy: 0.5612\n",
      "Epoch 156/200\n",
      "92/92 [==============================] - 24s 257ms/step - loss: 1.1684 - accuracy: 0.5585\n",
      "Epoch 157/200\n",
      "92/92 [==============================] - 25s 268ms/step - loss: 1.1632 - accuracy: 0.5640\n",
      "Epoch 158/200\n",
      "92/92 [==============================] - 24s 266ms/step - loss: 1.1666 - accuracy: 0.5560\n",
      "Epoch 159/200\n",
      "92/92 [==============================] - 24s 266ms/step - loss: 1.1569 - accuracy: 0.5632\n",
      "Epoch 160/200\n",
      "92/92 [==============================] - 25s 267ms/step - loss: 1.1586 - accuracy: 0.5628\n",
      "Epoch 161/200\n",
      "92/92 [==============================] - 23s 251ms/step - loss: 1.1654 - accuracy: 0.5610\n",
      "Epoch 162/200\n",
      "92/92 [==============================] - 23s 249ms/step - loss: 1.1710 - accuracy: 0.5596\n",
      "Epoch 163/200\n",
      "92/92 [==============================] - 23s 255ms/step - loss: 1.1540 - accuracy: 0.5691\n",
      "Epoch 164/200\n",
      "92/92 [==============================] - 25s 268ms/step - loss: 1.1670 - accuracy: 0.5598\n",
      "Epoch 165/200\n",
      "92/92 [==============================] - 24s 260ms/step - loss: 1.1762 - accuracy: 0.5563\n",
      "Epoch 166/200\n",
      "92/92 [==============================] - 25s 270ms/step - loss: 1.1718 - accuracy: 0.5582\n",
      "Epoch 167/200\n",
      "92/92 [==============================] - 24s 256ms/step - loss: 1.1661 - accuracy: 0.5615\n",
      "Epoch 168/200\n",
      "92/92 [==============================] - 23s 248ms/step - loss: 1.1562 - accuracy: 0.5645\n",
      "Epoch 169/200\n",
      "92/92 [==============================] - 24s 256ms/step - loss: 1.1700 - accuracy: 0.5554\n",
      "Epoch 170/200\n",
      "92/92 [==============================] - 24s 263ms/step - loss: 1.1700 - accuracy: 0.5570\n",
      "Epoch 171/200\n",
      "92/92 [==============================] - 25s 268ms/step - loss: 1.1637 - accuracy: 0.5591\n",
      "Epoch 172/200\n",
      "92/92 [==============================] - 24s 266ms/step - loss: 1.1646 - accuracy: 0.5600\n",
      "Epoch 173/200\n",
      "92/92 [==============================] - 25s 267ms/step - loss: 1.1648 - accuracy: 0.5602\n",
      "Epoch 174/200\n",
      "92/92 [==============================] - 24s 266ms/step - loss: 1.1607 - accuracy: 0.5617\n",
      "Epoch 175/200\n",
      "92/92 [==============================] - 25s 274ms/step - loss: 1.1578 - accuracy: 0.5645\n",
      "Epoch 176/200\n",
      "92/92 [==============================] - 25s 269ms/step - loss: 1.1618 - accuracy: 0.5611\n",
      "Epoch 177/200\n",
      "92/92 [==============================] - 24s 260ms/step - loss: 1.1543 - accuracy: 0.5663\n",
      "Epoch 178/200\n",
      "92/92 [==============================] - 24s 266ms/step - loss: 1.1713 - accuracy: 0.5570\n",
      "Epoch 179/200\n",
      "92/92 [==============================] - 26s 284ms/step - loss: 1.1594 - accuracy: 0.5600\n",
      "Epoch 180/200\n",
      "92/92 [==============================] - 24s 263ms/step - loss: 1.1638 - accuracy: 0.5592\n",
      "Epoch 181/200\n",
      "92/92 [==============================] - 24s 263ms/step - loss: 1.1645 - accuracy: 0.5613\n",
      "Epoch 182/200\n",
      "92/92 [==============================] - 24s 259ms/step - loss: 1.1730 - accuracy: 0.5547\n",
      "Epoch 183/200\n",
      "92/92 [==============================] - 24s 262ms/step - loss: 1.1718 - accuracy: 0.5613\n",
      "Epoch 184/200\n",
      "92/92 [==============================] - 23s 246ms/step - loss: 1.1547 - accuracy: 0.5615\n",
      "Epoch 185/200\n",
      "92/92 [==============================] - 23s 253ms/step - loss: 1.1538 - accuracy: 0.5635\n",
      "Epoch 186/200\n",
      "92/92 [==============================] - 25s 267ms/step - loss: 1.1745 - accuracy: 0.5548\n",
      "Epoch 187/200\n",
      "92/92 [==============================] - 24s 264ms/step - loss: 1.1633 - accuracy: 0.5624\n",
      "Epoch 188/200\n",
      "92/92 [==============================] - 24s 259ms/step - loss: 1.1678 - accuracy: 0.5605\n",
      "Epoch 189/200\n",
      "92/92 [==============================] - 25s 268ms/step - loss: 1.1560 - accuracy: 0.5626\n",
      "Epoch 190/200\n",
      "92/92 [==============================] - 25s 267ms/step - loss: 1.1607 - accuracy: 0.5625\n",
      "Epoch 191/200\n",
      "92/92 [==============================] - 24s 259ms/step - loss: 1.1512 - accuracy: 0.5664\n",
      "Epoch 192/200\n",
      "92/92 [==============================] - 23s 248ms/step - loss: 1.1674 - accuracy: 0.5580\n",
      "Epoch 193/200\n",
      "92/92 [==============================] - 23s 248ms/step - loss: 1.1511 - accuracy: 0.5623\n",
      "Epoch 194/200\n",
      "92/92 [==============================] - 23s 253ms/step - loss: 1.1582 - accuracy: 0.5639\n",
      "Epoch 195/200\n",
      "92/92 [==============================] - 26s 282ms/step - loss: 1.1697 - accuracy: 0.5586\n",
      "Epoch 196/200\n",
      "92/92 [==============================] - 24s 258ms/step - loss: 1.1611 - accuracy: 0.5574\n",
      "Epoch 197/200\n",
      "92/92 [==============================] - 23s 253ms/step - loss: 1.1593 - accuracy: 0.5612\n",
      "Epoch 198/200\n",
      "92/92 [==============================] - 24s 265ms/step - loss: 1.1497 - accuracy: 0.5634\n",
      "Epoch 199/200\n",
      "92/92 [==============================] - 24s 258ms/step - loss: 1.1731 - accuracy: 0.5545\n",
      "Epoch 200/200\n",
      "92/92 [==============================] - 23s 251ms/step - loss: 1.1516 - accuracy: 0.5660\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x19d5cdd8d48>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "\"\"\" history = model.fit_generator(datagen.flow(X_train,Y_train, batch_size=batch_size),\n",
    "                              epochs = epochs, validation_data = (X_val,Y_val), steps_per_epoch=X_train.shape[0] // batch_size) \"\"\"\n",
    "\n",
    "model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),epochs=epochs, verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction and Evalution of the model\n",
    "Now let's see how are model behaves and predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-- Training data --\n",
      "Accuracy: 70.61\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.63      0.65      3171\n",
      "           1       0.95      0.39      0.56       343\n",
      "           2       0.66      0.49      0.57      3276\n",
      "           3       0.80      0.90      0.85      5771\n",
      "           4       0.63      0.62      0.63      3901\n",
      "           5       0.82      0.78      0.80      2529\n",
      "           6       0.61      0.72      0.66      3976\n",
      "\n",
      "    accuracy                           0.71     22967\n",
      "   macro avg       0.74      0.65      0.67     22967\n",
      "weighted avg       0.71      0.71      0.70     22967\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1992    3  174  240  321   57  384]\n",
      " [  70  135   48   10   49    8   23]\n",
      " [ 323    0 1621  193  497  251  391]\n",
      " [ 102    0   81 5213  100   55  220]\n",
      " [ 273    1  203  299 2428   21  676]\n",
      " [  52    2  203  159   35 1971  107]\n",
      " [ 156    1  132  364  416   50 2857]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on training data\n",
    "print('\\n-- Training data --')\n",
    "predictions = model.predict(X_train)\n",
    "accuracy = sklearn.metrics.accuracy_score(y_train, np.argmax(predictions, axis=1))\n",
    "print('Accuracy: {0:.2f}'.format(accuracy * 100.0))\n",
    "print('Classification Report:')\n",
    "print(sklearn.metrics.classification_report(y_train, np.argmax(predictions, axis=1)))\n",
    "print('Confusion Matrix:')\n",
    "print(sklearn.metrics.confusion_matrix(y_train, np.argmax(predictions, axis=1)))\n",
    "print('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---- Test data ----\n",
      "Accuracy: 55.83\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.43      0.46       824\n",
      "           1       0.88      0.25      0.39        93\n",
      "           2       0.42      0.32      0.36       821\n",
      "           3       0.71      0.80      0.75      1444\n",
      "           4       0.42      0.43      0.43       929\n",
      "           5       0.70      0.66      0.68       642\n",
      "           6       0.49      0.60      0.54       989\n",
      "\n",
      "    accuracy                           0.56      5742\n",
      "   macro avg       0.59      0.50      0.51      5742\n",
      "weighted avg       0.56      0.56      0.55      5742\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 354    1   69   85  132   31  152]\n",
      " [  16   23   10    7   26    1   10]\n",
      " [ 114    2  260   79  157   85  124]\n",
      " [  54    0   33 1158   69   33   97]\n",
      " [  96    0  103  116  397   12  205]\n",
      " [  13    0   92   64   17  423   33]\n",
      " [  58    0   53  129  140   18  591]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate on test data\n",
    "print('\\n---- Test data ----')\n",
    "predictions = model.predict(X_test)\n",
    "accuracy = sklearn.metrics.accuracy_score(y_test, np.argmax(predictions, axis=1))\n",
    "print('Accuracy: {0:.2f}'.format(accuracy * 100.0))\n",
    "print('Classification Report:')\n",
    "print(sklearn.metrics.classification_report(y_test, np.argmax(predictions, axis=1)))\n",
    "print('Confusion Matrix:')\n",
    "print(sklearn.metrics.confusion_matrix(y_test, np.argmax(predictions, axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i don't know yet what this whole means, but i can say that is not working ! let's try to use the ImageGenerator this time\n",
    "\n",
    "allora, innanzitutto vedo che train accuracy e molto piu grande di test accuracu, quindi il modello ha overfittato un po, poi comunque training accuracy di 66% e abbastanza bassa, mentre quella del test di 54 non va proprio bene... guardando gli F1 score vedo che le classi 3 e 5 sono quelle predette meglio ( f1 score piu alto), dove la classe 3 e quella piu popolosa ( support di 5771 valori per training e 1444 per testing) e quindi ha senso che il modello abbia predetto meglio visto che ha avuto piu immagini di training, mentre e interessante come la classe 5 ha avuto predizioni migliori della classe 0,2,4,6 sebbene abbia meno immaigni di training ( \"solo\" 2529)... quindi si vede che le immagini di classe 5, sebbene poche, si distringuono bene da quelle delle altre classi\n",
    "\n",
    "\n",
    "infine vedo subito che la classe 1 e quella che da risultati peggiori visto che ha un supporto decisamente tanto sbilanciato\n",
    "\n",
    "L\n",
    "quasi un decimo delle altre classi...\n",
    "\n",
    "quindi, sicuramente il modello va migliorato\n",
    "\n",
    "smanetta un po con altre CNN + ANN che trovi ( tipo quella che stai gia usando)\n",
    "\n",
    "se lo alleno, pi tempo passa e pi training il set funziona. Dovrei lasciarlo fare per alemeno 1000 epoche. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0bc47dd249f7da4cbfd7c9052535130193f76861efa818615cbe728367195f1c"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('ludo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
